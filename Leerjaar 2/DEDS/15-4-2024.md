# Neural Networks (deel 1)
Hoorcollege wordt mogelijk opgenomen en er komt ook nog een online hoorcollege.

## Lesdoelen
### HC-1
- Uitleggen wat een Neural Network  (NN) is
- Welke componenten een NN heeft
- Hoe je een NN traint
- Hoe een NN een voorspelling maakt

### WC
- Bouwen van een kleine NN

Extra uitdaging: het trainen van het NN.

Neurale netwerken bestaan al lang.

## Wat is een NN?
Eigenlijk een digitaal brein, het krijgt input, het doet berekeningen ermee en geeft een output. Dit is vaak een voorspelling.

## Wat kan je met een NN?
Inzichten en patronen uit een grote dataset halen en er geen linieaire verbanden zijn tussen de datapunten. Je kunt input classificeren, voorspellingen maken, en nieuwe data genereren.

Een Neural network benadert een functie, na het trainen is het eigenlijk een wiskundige functie.

Een neural network is een collectie lagen. Elke laag bestaat uit nodes (neuronen). Elke node van een laag is met elke node van de volgende laag verbonden. De eerste laag is de input-laag en de laatste is de voorspelling. Daar tussenin zijn de verborgen lagen, die doen het werk.

We gaan in principe bij een neural network altijd van links naar rechts.

## Waar komt een kunstmatig NN vandaan?
Het is gebaseerd op hoe het menselijk brein werkt. De inputs zijn je zintuigen, en de output zijn de impulsen die naar je spieren gaan.

Synapsen en connecties:
- Onderdeel van de verbindingen, een synaps heeft een variabele sterkte. Dit wordt samen met de axon tak en de dendrite tak een `weight`/connectie in de context van een kunstmatig neural network. De weight is een getal wat vaak tussen de 0 en de 1 is<sup>[1]</sup>.

Een neural network is een universele functie-benaderaar. Het kan elke functie benaderen. Een functie is iets wat een input pakt en een output geeft. Een neural network mapt input-waardes naar output waardes.

Vaak neemt een neural network meer dan 1 input mee en dan benaderd het een hyper-dimensionale functie.

## Bouwstenen en berekeningen
- Neuronen/nodes: een rondje in je netwerk
- Connectie: lijn tussen nodes
- Input layer: de eerste laag van nodes, hier stop je je input in
- Output layer: waar je output uit komt
- Hidden layer: een laag tussen de input layer en de output layer.
## Activatiefunctie
Een neuron heeft een activatie. Elke node uit de vorige laag heeft een activatie, die gaat keer de weight. De `activatie * weight` komt binnen voor elk van de nodes uit de vorige laag en wordt bij elkaar opgeteld<sup>[2]</sup>. Deze uitslag wordt door een speciale functie gehaald<sup>[3]</sup>. Deze functie is de activatiefunctie die zorgt dat de activatie tussen de 0 en de 1 blijft.

### Sigmoid
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

## Trainen
Het netwerk weet in eerste instantie niks, je geeft het gelabelde data, en het gebruikt deze om te trainen<sup>[4]</sup>.

Je vergelijkt de output van je netwerk met de gewenste output. Dan weetje wat de error is, en dan verander je de weights van het netwerk<sup>[4]</sup>.

Het trainen moet je nu nog als black box zien. Hierin worden de weights aangepast.

Bij het trainen pas je de weights aan om het netwerk te verbeteren. Een enkele training-iteratie gaat door alle data heen in de dataset.

```
Neem 1 record uit de database als input
VOer de input door het NN uit, van links naar rechts
Bereken de output
Bereken de error (verschil tussen output en juiste antwoord)
Pas connecties aan
Herhaal voor alle datapunten
```

Er zijn meerdere manieren om te trainen, zoals backpropagation en gradient descent. Je kan ook random weights genereren, dat is makkelijker dan backpropagation en gradient descent. Het is alleen minder efficient omdat de aanpassing niet gerelateerd is aan wat er eigenlijk aangepast moet worden.

Hill climbing (niet te verwarren met Hill Climb Racing) is ook een manier: je probeert kleine aanpassingen te maken om het netwerk te verbeteren. Dit lijk heel erg op gradient descent<sup>5</sup>.

### Overfitting
Als je te veel aan het trainen bent. Het resultaat is dat je netwerk je training-data "onthoud" en geen idee heeft hoe die moet generaliseren. Hoe langer je traint, hoe beter die de training-data voorspelt, maar als je te veel traint, dan zal het minder goed generaliseren naar de testdata.

Je herkent overfitten aan een hoge accuracy op de training-data en een lage accuracy op de test-data.

Er wordt ook een validation dataset gebruikt, dat is deel van de training-set (het wordt bij het trainen gebruikt). Je moet het trainen stoppen zodra de error op de validation-data minimaal is. De error op de training-set zal toch wel omlaag gaan. Je gaat daarna de test-dataset gebruiken om de kwaliteit van de netwerk te bepalen. De validation set bepaald wanneer je moet stoppen met trainen.

Je moet nog wel testen met een andere set dan de validation set, want de validation set wordt wel gebruikt om te bepalen wanneer het netwerk moet stoppen met trainen.

## Inputs en outputs
Input is gewoon de data waar je een voorspelling op wil doen, de output is je voorspelling.

We mogen LLM gebruiken voor het maken van een neural network. Dus dat ga ik zo meteen gelijk doen.

**Er staat een opdracht op BrightSpace. Je mag het met Generative AI maken. Je mag het ook in Python doen, maar de docent raad dat niet aan.**

# Footnotes (extra informatie die ik zelf heb toegevoegd)
[1] Afhankelijk van architectuur. Soms kan een weight negatief zijn.

[2] Eigenlijk wordt er meestal nog een (mogelijk negatieve) bias bijopgeteld.

[3] Meestal de sigmoid functie.

[4] Met backpropagation.

[5] Alleen dan zonder de slopes te hoeven berekenen.
