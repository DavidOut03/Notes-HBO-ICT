# Machine learning modellen
- Regressiemodellen
- Classificatiemodellen
- Supervised modellen
- Unsupervised modellen

# Dataset preparatie voor ML
Dit kost 70 % tot 75 % van de tijd. Je maakt de data schoon, en haalt niet nummerieke data eruit. Ook kies je wat voor model je nodig hebt.

# Verschil tussen ML en AI
AI maakt van regels en data een antwoord. ML maakt van data en antwoorden de regels.

# Machinelearning modellen
## Supervised
### Regressie
Voor continue, numerieke data. Een lijn door de data heen trekken.

Lineaire regressie: `Y = aX + b`
`a` is de slope, en `b` is de intersept.

Meestal heb je een veeldimensionale dataset, dan valt er dus best veel onder X en wordt dit de formule:

$Y = \sum [a_n X_n] + b$

### Classificatie
Voor discrete data.
## Unsupervised
### Clustering
### Association

# CRISP DM: Van vraag naar ML antwoord
Business understanding <-> Data understanding -> Data preparation <-> Modelling -> Evalutation -> Deployment / Terug naar business understanding.

## Evaluation
Hoe goed doet mijn model het nu eigenlijk?

## Deployment
Dat is feest, je project is klaar.

## Data preparation
Dat is waar je al probeert patronen in de data te vinden, ook doe je hier de train/test split (horizontaal snijden), en bepaal je wat je wil voorspellen (vertikaal snijden).

## Modelling
Hier kies je wat voor model je gaat gebruiken.

# Verschil tussen supervised learning en unsupervised learning
## Supervised
Data + antwoord -> regels

Vereist gelabelde data.
## Unsupervised
Data -> regels

Voor als je geen antwoord hebt. Vereist geen gelabelde data. Hoe goed het model het doet is lastig tot niet te controleren. Je gaat er eigenlijk vanuit dat het model gelijk heeft. Als je het wel zou kunnen controleren, dan zou het supervised learning zijn.

# Sklearn
Hiermee kan je regressie doen, errors berekenen, en meer. Met `get_dummies` kan je one hot encoden.

`train_test_split` geeft je de `x_train`, `y_train`, `x_test` en `y_test`. Je kan een testsize meegeven om te kiezen hoe groot de testset is.

Hieronder volgt een voorbeeld van de `get_dummies` methode, en hoe `train_test_split` en `LineairRegression.fit` werken. Ook ziet u daar hoe je `model.predict` gebruikt en hoe `matplotlib` kan worden gebruikt om te visualiseren.

```py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Example code for get_dummies
data = pd.DataFrame({'color': ['red', 'blue', 'green']})
encoded_data = pd.get_dummies(data)

# Example code for train_test_split and LinearRegression.fit
X = [[1], [2], [3], [4], [5]]
y = [2, 4, 6, 8, 10]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)

# Example code for model.predict and matplotlib visualization
y_pred = model.predict(X_test)
plt.scatter(X_test, y_test, color='blue')
plt.plot(X_test, y_pred, color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.show()
```

Opdracht voor WC staat op BS.

# Testbaar en trainbaar maken van modellen
85 % traindata, 15 % testdata, willekeurig splitsen.

`X` is input, `Y` is output. `train` is train-data, `test` is testdata, dat maakt 4 verschillende variabelen `X_train`, `Y_train`, `X_test` en `Y_test`.

Je wil vaak afhankelijke variabelen voorspellen op basis van onafhankelijke variabelen.

Vertikaal snijden: bepalen wat je wil voorspellen.

Horizontaal snijden: bepalen wat traindata en wat testdata is.

# Evalueren van het model
## Sum of Squared Errors (SSE)
$SSE = \sum{e_n^2}$
## Mean of Squared Errors (MSE)
$MSE = \frac{\sum{e_n^2}}{count}$
## Mean of Absolute Errors (MAE)
$MAE = \frac{\sum{|e_n|}}{count}$

Machine learning modellen zoeken naar patronen in de data, ook als de data eigenlijk geen verband heeft. Bij ML gaat het altijd om voorspellen van data. Op 1 dataset kan je vaak meerdere keuzes maken wat je afhankelijke variabele is. Als je een categorische dataset hebt, gebruik je geen regressiemodel, maar een classificatiemodel. Regressie vereist veel datapreparatie, ook omdat het allemaal nummerieke data moet zijn.

One hot encoding: Per categorie een eigen kolom met een boolean die je als 0 of 1 interpreteert (een numerieke waarde). Doet dit niet als een waarde per rij verschilt.
