# Unsupervised Machine Learning
Hetzelfde als supervised machine learning, maar dan unsupervised. Supervised voorspellingen kan je op de waarheid controleren, unsupervised technieken (zoals clustering technieken) kan je niet op de waarheid controleren.

## Mededelingen
- Rooster bevat mogelijk een fout, het hoorcollege van donderdag en vrijdag van Jelle komt morgen online. Dit is wegens familieredenen van Jelle.

## Clusteringmodellen in unsupervised machine learning
Clusteringmodellen kunnen op niet willekeurige basis groeperen. Je kan wel de kwaliteit van het clusteringsmodellen vaststellen door afstand te bepalen.

## Classificatie vs Clustering
Classificatie is als je labels hebt (je weet in welke klasse een datapunt hoort), clustering als je die niet hebt.

In beide gevallen worden datapunten in klasses ingedeeld. Bij een classficatiemodel heb je gelabelde data, bij een clusteringmodel heb je ongelabelde data.

### Voordeel clustering
- Creatiever in het vinden van clusters.

### Nadeel clustering
- Mogelijk inaccuraat.

### Voordeel classificatie
- Resultaten zijn makkelijker te meten.

### Nadeel classificatie
- Minder creatief.

## Toepassing clustering
Elke situatie waar je data niet gelabel is. Dus je weet niet welke klassen er zijn. ChatGPT gebruikt ook clustering om de betekenis van een token te bepalen.

## Overlapping clustering
Een datapunt hoort mogelijk bij meerdere clusters.

## Non-overlapping clustering
Een datapunt hoort maar bij 1 cluster.

## Top-down vs Bottom-up
### Top-down clustering
Begin met de datapunten, en maak clusters daarvan.
### Bottom-up clustering
Elk datapunt krijgt een cluster, verschillende clusters worden samengevoegd met dichtbijzijnde clusters.

## Stappen van een k-meansclusteringsmodel
K-means is slechts 1 van de manieren om een clusteringmodel te draaien.

Hoe kies je het aantal clusters (de waarde van K)? K staat voor het aantal clusters. Je weet vaak niet wat de waarde van K moet worden. Haal een waarde van K uit de hoge hoed.

```py
import pandas as pd
from sklearn.cluster import kMeans
import math
import matplotlib.pyplot as plt
import warnings
import numpy as np


warnings.simplefilter('ignore') # Irritante waarschuwingen uitzetten


df = pd.read_csv("titanic.csv", sep=";")
df = df.loc[:, ["Pclass", "Sex", "Age", "Survived"]]

# One-hot encoding
df["Pclass"] = df["Pclass"].astype(str)
dummies_dataframe = pd.get_dummie(df.loc[:, ["Sex", "Pclass"]])     # Pclass is niet continue, je kan niet in klas 2.5 zitten. Dus het is een categorische/discrete waarde.

...

df = pd.concat(...)

...

df_2d = df.loc[:, ["Age", "Survived"]]

kmeans = KMeans(n_clusters = 3, random_state = 42) # n_clusters is de equivalent van k in k-means.
kmeans.fit_predict(df_2d)

kmeans_centra = pd.DataFrame(kmeans.cluster_centers_)

for i in range(len(kmeans_centra.columns)):
    kmeans_centra = kmeans_centra.rename(columns = {i: f"{df_2d.columns[i]}"})


for scr_index, _ in df_2d.iterrows():
    euclidian_distances = dict()
    print(f"Afstand van brondindex {src_index} tot...")

    for centrumindex, _ in kmeans_centra.iterrows():
        print(f"\tCentrumindex {centrumindex}:")
        euclidian_cum = 0

        ...


df_2d.groupby("Centrum", as_index=False)["Centrum"].count()     # Geeft het aantal datapunten per centrum. Als je goed hebt geclusterd als de cluster-grootte ongeveer even groot is als een klasse die je verwacht.

kmeans = KMeans(n_clusters=6, random_state=42)
prediction_results = kmeans.fit_predict(df)     # Dit is hyperdimensionale data
```

## Kwaliteitberekeningen
### Interclusterafstand
De gemiddelde afstand tussen clusters, die wil je zo groot mogelijk hebben.
### Intraclusterafstand
De gemiddelde afstand tussen datapunten in een cluster, die wil je zo klein mogelijk hebben.

Denk aan **inter**net, de **inter**clusterafstand is dus de afstand tussen netwerken op het **inter**net.

Als je overlappende clusters hebt, dan is de intraclusterafstand belangrijker. Je hebt ook hybride clusteringmodellen (dat een deel overlapt en een deel niet).

Advies van Jelle: experimenteer met meerdere waardes van K. K is een verbetering als de interclusterafstand hoger is en de intraclusterafstand verlaagd is.

Bij een clusteringmodel evalueer je niet de kloppendheid van je model, wel de kwaliteit van je model (aan de hand van de interclusterafstand en de intraclusterafstand).

## Clustering
1. Kies een waarde voor K. Voor dit voorbeeld 3.
2. Je kiest k startcentra (in dit geval kies je dus 3 punten).
3. Elk punt langslopen en koppelen aan het dichstbijzijnde cluster. Euclidische afstand (vernoemd naar een wiskundige), gewoon de afstand tussen twee punten als je een lineaal op je papier zou leggen tussen de twee punten.
4. Zoek nu van elk gemaakt cluster het datapunt wat in het midden zit.
5. Nu kan je terug naar stap 3 tot je centra (bijna) niet meer verplaatsen. Het zal bijna nooit zo zijn dat de verplaatsing helemaal 0 zal worden.

# Classificatiemodellen
## Decision tree
Beslissing in elk knooppunt.
## Random forests
Combineert meerdere decisiontrees. Uitbreiding van de decision tree. Veel bomen bij elkaar? Een bos.
## Logistische regressie
Modelleert de waarschijnlijkheid van uitkomsten.
# Clustering
## K-means
Zie eerder in dit document.
## Hierarchisch
CLusterhierarchienen maken door op basis va inter- en intraclusterafstand clusters samen te voegen.
## DBSCAN
Clusters op basis van datadichtheid bepalen (aantal punten, etc).
