Frequent Itemsets
Association leer je het beste door het te doen, het is een vorm van unsupervised learning.

Wat is het? Een set die bepaalde welke brondatarijen vaak samen voorkomen, dit kan je met A-priori doen. Je gebruik selectiecriteria.

Frequent itemset: verzameling rijen die vaak samen voorkomen in de dataset. Zoals dat koffie en melk vaak samen worden verkocht.

Bij een grote dataset kan je niet handmatig frequent itemsets vinden, dan moet het echt automatisch.

Uit frequent itemsets kan je association rules halen. Elke rij wordt met elke andere rij gecombineerd bij dit algotime. Support en confidence zijn de twee drempelwaardes die je handmatig kunt kiezen.

Noodzakelijke gegevens:
- Alles waarvan je de mate dat ze samen gaan wil achterhalen

Frequent itemsets bestaat uit:
- Kunstmatige sleutel (surrogate key)
- Voor vaak voorkomende items

Vaak kanj je deze sets verklaren, support en confidence wordt gebruikt om te bepalen welke waardes positive zijjn.

FIS kan je vaak met dummy encoding gebruiken.

```py
import pandas as pd

pd.get_dummies(df)
```

# Kengetallen
Eerst verzamel je van willekeurige itemsets combinaties van 2, 3 etc. Dus je moet finetunen.

Support count berekenen:

$$
\frac{rijen met combo}{totale rijen} = support count
$$

Confidence berekenen:

...

Minsup threshhold: minimum support count (FIS).

Non-frequent itemsets = itemsets die niet frequent zijn.

Association rules worden vaak als synoniem gebruikt voor FIS, maar Jelle zijn association rules juist je conclusies, dus je causaliteit.

Association rules hebben ook een confidence (percentage dat het klopt gegeven de aanname) en een support (percentage dat het klopt).

Association rule mining: zoeken naar association rules. Je verandert je aanname en kijkt of je met een hogere confidence een conclusie kan trekken.

Association rule brute force: alle mogelijke association rules afgaan om de beste te vinden. Dit kost $O(2^d)$

Genereer frequent itemsets van lengte k:
- Start bij $k=1$
- Herhaal tot je niks meer vind:
  - Genereer $k+1$ kandidaatsets met lengte $k$ FIS
  - Snoei de kandidaten die een subset van lengte $k$ bevatten die infrequent zijn
  - Bereken support
  - Verwijder infrequent itemsets

Complexiteitsfactoren:
- Minimum support
- Aantal items
- Aantal transacties
- Grootte van de transacties

Iteratief de treshhold, etc aan te passen kan stuff verbeteren.

Als $\{a, b, c\}$ een FIS ism dan zijn er veel candidate rules: $2^k-2$ opties om precies te zijn.

Als $c(ABC \rarr D) \ge c(AB \rarr CD) \ge c(A \rarr BCD)$ dan heb je een goede rule.

Closed itemset: geen superset met dezelfde support.

Maximal frequent itemsset: als geen superset frequentie is.

De slide `Overgebleven Frequent Itemsets` is mij niet duidelijk, dus die negeer ik.
